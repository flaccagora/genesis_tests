{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5afcdc",
   "metadata": {},
   "source": [
    "# Scene Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ea838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 11/17/25 14:24:39.168 50713] [shell.py:_shell_pop_print@25] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;159m[Genesis] [14:24:40] [INFO] \u001b[38;5;121mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:40] [INFO] \u001b[38;5;121mâ”‚â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆ\u001b[0m\u001b[38;5;159m \u001b[38;5;121m\u001b[1m\u001b[3mGenesis\u001b[0m\u001b[38;5;159m \u001b[38;5;121mâ”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‚\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:40] [INFO] \u001b[38;5;121mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:41] [INFO] Running on \u001b[38;5;121m\u001b[4m[NVIDIA GeForce GTX 1650 Ti]\u001b[0m\u001b[38;5;159m with backend \u001b[38;5;121m\u001b[4mgs.cuda\u001b[0m\u001b[38;5;159m. Device memory: \u001b[38;5;121m\u001b[4m3.63\u001b[0m\u001b[38;5;159m GB.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:41] [INFO] ğŸš€ Genesis initialized. ğŸ”– version: \u001b[38;5;121m\u001b[4m0.3.7\u001b[0m\u001b[38;5;159m, ğŸ¨ theme: \u001b[38;5;121m\u001b[4mdark\u001b[0m\u001b[38;5;159m, ğŸŒ± seed: \u001b[38;5;121m\u001b[4m0\u001b[0m\u001b[38;5;159m, ğŸ› debug: \u001b[38;5;121m\u001b[4mFalse\u001b[0m\u001b[38;5;159m, ğŸ“ precision: \u001b[38;5;121m\u001b[4m32\u001b[0m\u001b[38;5;159m, ğŸï¸ performance: \u001b[38;5;121m\u001b[4mFalse\u001b[0m\u001b[38;5;159m, ğŸ’¬ verbose: \u001b[38;5;121m\u001b[4mINFO\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:44] [INFO] Scene \u001b[38;5;121m\u001b[3m<49e4263>\u001b[0m\u001b[38;5;159m created.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:44] [INFO] Adding \u001b[38;5;121m<gs.RigidEntity>\u001b[0m\u001b[38;5;159m. idx: \u001b[38;5;121m0\u001b[0m\u001b[38;5;159m, uid: \u001b[38;5;121m\u001b[3m<8d79080>\u001b[0m\u001b[38;5;159m, morph: \u001b[38;5;121m<gs.morphs.Plane>\u001b[0m\u001b[38;5;159m, material: \u001b[38;5;121m<gs.materials.Rigid>\u001b[0m\u001b[38;5;159m.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:44] [INFO] Adding \u001b[38;5;121m<gs.FEMEntity>\u001b[0m\u001b[38;5;159m. idx: \u001b[38;5;121m1\u001b[0m\u001b[38;5;159m, uid: \u001b[38;5;121m\u001b[3m<9c38fd2>\u001b[0m\u001b[38;5;159m, morph: \u001b[38;5;121m<gs.morphs.Mesh(file='/home/flaccagora/Desktop/test/genesis_tests/assets/dragon.obj')>\u001b[0m\u001b[38;5;159m, material: \u001b[38;5;121m<gs.FEM.Muscle>\u001b[0m\u001b[38;5;159m.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:45] [INFO] Building scene \u001b[38;5;121m\u001b[3m<49e4263>\u001b[0m\u001b[38;5;159m...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:48] [INFO] Entity <9c38fd2> added. class: FEMEntity, morph: Mesh, size: (78734, 18478), material: <gs.FEM.Muscle>.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:24:49] [INFO] Compiling simulation kernels...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:25:10] [INFO] Building visualizer...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:25:11] [INFO] Viewer created. Resolution: \u001b[38;5;121m800Ã—600\u001b[0m\u001b[38;5;159m, max_FPS: \u001b[38;5;121m60\u001b[0m\u001b[38;5;159m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import genesis as gs # type: ignore\n",
    "\n",
    "def gs_simul_setup(entity_name):\n",
    "    ########################## init ##########################\n",
    "    gs.init(seed=0, precision='32', logging_level='info')\n",
    "\n",
    "    dt = 5e-4\n",
    "    scene = gs.Scene(\n",
    "        sim_options=gs.options.SimOptions(\n",
    "            substeps=10,\n",
    "            gravity=(0, 0, 0),\n",
    "        ),\n",
    "        viewer_options= gs.options.ViewerOptions(\n",
    "            camera_pos=(1.5, 0, 0.8),\n",
    "            camera_lookat=(0.0, 0.0, 0.0),\n",
    "            camera_fov=40,\n",
    "        ),\n",
    "        mpm_options=gs.options.MPMOptions(\n",
    "            dt=dt,\n",
    "            lower_bound=(-1.0, -1.0, -0.2),\n",
    "            upper_bound=( 1.0,  1.0,  1.0),\n",
    "        ),\n",
    "        fem_options=gs.options.FEMOptions(\n",
    "            dt=dt,\n",
    "        ),\n",
    "        vis_options=gs.options.VisOptions(\n",
    "            show_world_frame=False,\n",
    "        ),\n",
    "        show_viewer=True,\n",
    "    )\n",
    "\n",
    "    ########################## entities ##########################\n",
    "    # scene.add_entity(morph=gs.morphs.Plane())\n",
    "\n",
    "    E, nu = 3.e4, 0.45\n",
    "    rho = 1000.\n",
    "\n",
    "\n",
    "    torus_fem_0 = scene.add_entity(\n",
    "        morph=gs.morphs.Mesh(\n",
    "            file=f'assets/{entity_name}.obj',\n",
    "            pos=(0.5, 1, 0.3),\n",
    "            scale=0.2,\n",
    "            ),\n",
    "        material=gs.materials.FEM.Muscle(\n",
    "            E=E,\n",
    "            nu=nu,\n",
    "            rho=rho,\n",
    "            model='stable-neohooken',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # torus_fem_1 = scene.add_entity(\n",
    "    #     morph=gs.morphs.Mesh(\n",
    "    #         file='assets/Torus.obj',\n",
    "    #         pos=(0.5, 0.4, 0.3),\n",
    "    #         scale=0.2,\n",
    "    #         ),\n",
    "    #     material=gs.materials.FEM.Muscle(\n",
    "    #         E=E,\n",
    "    #         nu=nu,\n",
    "    #         rho=rho,\n",
    "    #         model='stable-neohooken',\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "\n",
    "    # cam = scene.add_camera(\n",
    "    #     res    = (640, 480),\n",
    "    #     pos    = (7., 0.4, 0.3), # (3,,) per torus is enough\n",
    "    #     lookat = (0.5, 0.4, 0.3),\n",
    "    #     fov    = 30,\n",
    "    #     GUI    = False,\n",
    "    # )\n",
    "\n",
    "    cam = None\n",
    "    ########################## build ##########################\n",
    "    scene.build()\n",
    "\n",
    "    return scene, cam\n",
    "\n",
    "scene, cam = gs_simul_setup(\"dragon\")\n",
    "torus_fem_0 = scene.entities[1]\n",
    "# torus_fem_1 = scene.entities[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5293b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REMEMBER TO ALWAYS ROTATE FROM A REFERENCE FRAME POSITION\\nOTHERWISE THE ROTATION WILL ACCUMULATE ERRORS'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rotation_matrix_xyz(rx, ry, rz):\n",
    "    Rx = torch.tensor([[1, 0, 0],\n",
    "                   [0, torch.cos(rx), -torch.sin(rx)],\n",
    "                   [0, torch.sin(rx), torch.cos(rx)]], dtype=torch.float32)\n",
    "    \n",
    "    Ry = torch.tensor([[torch.cos(ry), 0, torch.sin(ry)],\n",
    "                   [0, 1, 0],\n",
    "                   [-torch.sin(ry), 0, torch.cos(ry)]], dtype=torch.float32)\n",
    "    \n",
    "    Rz = torch.tensor([[torch.cos(rz), -torch.sin(rz), 0],\n",
    "                   [torch.sin(rz), torch.cos(rz), 0],\n",
    "                   [0, 0, 1]], dtype=torch.float32)\n",
    "    \n",
    "    R = Rz @ Ry @ Rx\n",
    "    return R\n",
    "\n",
    "def rotate_entity(entity, rx, ry=None, rz=None, center=None):\n",
    "    if rx.shape == torch.Size([1,3]):\n",
    "        R = rotation_matrix_xyz(rx[0,0], rx[0,1],rx[0,2])\n",
    "    elif ry == None or rz == None and rx.shape == torch.Size([3,3]):\n",
    "        R = rx\n",
    "    elif (ry is not None) and (rz is not None):\n",
    "        R = rotation_matrix_xyz(rx, ry, rz)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    state = entity.get_state()\n",
    "    pos = state.pos\n",
    "    if center is not None:\n",
    "        com = center\n",
    "    else:   \n",
    "        com = pos.mean(dim=1)\n",
    "    pos_centered = pos - com\n",
    "    pos_rotated = pos_centered @ R.T + com\n",
    "    entity.set_position(pos_rotated.sceneless())\n",
    "\n",
    "\n",
    "\"\"\"REMEMBER TO ALWAYS ROTATE FROM A REFERENCE FRAME POSITION\n",
    "OTHERWISE THE ROTATION WILL ACCUMULATE ERRORS\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaca6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m[Genesis] [14:25:40] [WARNING] Manally setting element positions. This is not recommended and could break gradient flow.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:25:40] [INFO] Running at \u001b[38;5;121m0.18\u001b[0m\u001b[38;5;159m FPS.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "rotation = torch.tensor([[torch.pi/3,torch.pi/2,0.]])\n",
    "scene.reset()\n",
    "center=torch.tensor([0.5, 0.1, 0.3])\n",
    "rotate_entity(torus_fem_0,rotation,center)\n",
    "scene.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a005c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation = torch.tensor([[0.,0.,0.]])\n",
    "rotation.shape == torch.Size([1,3])\n",
    "# rotation[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "796cda0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_matrix_xyz(torch.tensor([0.]),torch.tensor([0.]),torch.tensor([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3637fd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m[Genesis] [14:12:02] [WARNING] Manally setting element positions. This is not recommended and could break gradient flow.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [14:12:02] [INFO] Running at \u001b[38;5;121m0.06\u001b[0m\u001b[38;5;159m FPS.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "scene.reset()\n",
    "state = torus_fem_0.get_state()\n",
    "pos = state.pos\n",
    "rotated = pos @ rotation_matrix_xyz(torch.tensor([0.]),torch.tensor([0.]),torch.tensor([0.])).T\n",
    "assert torch.allclose(rotated,pos)\n",
    "\n",
    "torus_fem_0.set_position(rotated.sceneless())\n",
    "scene.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53a289",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddcbaf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REMEMBER TO ALWAYS ROTATE FROM A REFERENCE FRAME POSITION\\nOTHERWISE THE ROTATION WILL ACCUMULATE ERRORS'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rotation_matrix_xyz(rx, ry, rz):\n",
    "    Rx = torch.tensor([[1, 0, 0],\n",
    "                   [0, torch.cos(rx), -torch.sin(rx)],\n",
    "                   [0, torch.sin(rx), torch.cos(rx)]], dtype=torch.float32)\n",
    "    \n",
    "    Ry = torch.tensor([[torch.cos(ry), 0, torch.sin(ry)],\n",
    "                   [0, 1, 0],\n",
    "                   [-torch.sin(ry), 0, torch.cos(ry)]], dtype=torch.float32)\n",
    "    \n",
    "    Rz = torch.tensor([[torch.cos(rz), -torch.sin(rz), 0],\n",
    "                   [torch.sin(rz), torch.cos(rz), 0],\n",
    "                   [0, 0, 1]], dtype=torch.float32)\n",
    "    \n",
    "    R = Rz @ Ry @ Rx\n",
    "    return R\n",
    "\n",
    "def rotate_entity(entity, rx, ry=None, rz=None, center=None):\n",
    "    print(rx.shape)\n",
    "    if ry == None or rz == None:\n",
    "        R = rx\n",
    "    elif rx.shape == (1,3):\n",
    "        print(\"stomodificando\")\n",
    "        R = rotation_matrix_xyz(rx[0,0], rx[0,1],rx[0,2])\n",
    "    else:\n",
    "        R = rotation_matrix_xyz(rx, ry, rz)\n",
    "\n",
    "    print(R)\n",
    "    state = entity.get_state()\n",
    "    pos = state.pos\n",
    "    if center is not None:\n",
    "        com = center\n",
    "    else:   \n",
    "        com = pos.mean(dim=1)\n",
    "    pos_centered = pos - com\n",
    "    pos_rotated = pos_centered @ R.T + com\n",
    "    entity.set_position(pos_rotated.sceneless())\n",
    "\n",
    "\n",
    "\"\"\"REMEMBER TO ALWAYS ROTATE FROM A REFERENCE FRAME POSITION\n",
    "OTHERWISE THE ROTATION WILL ACCUMULATE ERRORS\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "!mkdir -p dataset\n",
    "n = 10\n",
    "for f1 in range(n):\n",
    "    for f2 in range(n):\n",
    "        for f3 in range(n):\n",
    "            angle = torch.tensor([torch.pi * f1 / n, torch.pi * f2 / n, torch.pi * f3 / n])\n",
    "            scene.reset()\n",
    "            rotate_entity(torus_fem_0, angle[0], angle[1], angle[2], center=None)\n",
    "            scene.step()\n",
    "            # save image and rotation matrix\n",
    "            img = np.array(cam.render()[0])\n",
    "            # R = rotation_matrix_xyz(angle[0], angle[1], angle[2])\n",
    "            R = torch.tensor([angle[0], angle[1], angle[2]])\n",
    "            # save img and R to disk\n",
    "            img_filename = f\"dataset/image_f1_{f1}_f2_{f2}_f3_{f3}_n_{n}.npy\"\n",
    "            R_filename = f\"dataset/rotation_f1_{f1}_f2_{f2}_f3_{f3}_n_{n}.th\"\n",
    "            np.save(img_filename, img)\n",
    "            torch.save(R, R_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fea61",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4116da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "class DeformNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeformNet, self).__init__()\n",
    "        self.set_feature_extractor()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 9)  # Output 3x3 rotation matrix\n",
    "    \n",
    "    def set_feature_extractor(self):\n",
    "        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "        self.dinov2 = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False  # Freeze the feature extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = self.processor(images=x, return_tensors=\"pt\")\n",
    "        outputs = self.dinov2(**inputs)\n",
    "        x = outputs.last_hidden_state  # (batch_size, seq_len, feature_dim)\n",
    "        x = torch.mean(x, dim=1)  # Global average pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 3, 3)  # Reshape to 3x3 matrix\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ccfe119",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a 'cuda' device type for generator but found 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n\u001b[32m     76\u001b[39m dataloader = create_dataloader(\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m, batch_size=\u001b[32m8\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch of images shape:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch of rotation matrices shape:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:787\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    788\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:722\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/data/sampler.py:354\u001b[39m, in \u001b[36mBatchSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m [*batch_droplast]\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     batch = [*itertools.islice(sampler_iter, \u001b[38;5;28mself\u001b[39m.batch_size)]\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m batch:\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/data/sampler.py:195\u001b[39m, in \u001b[36mRandomSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples // n):\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m torch.randperm(n, generator=generator).tolist()[\n\u001b[32m    197\u001b[39m         : \u001b[38;5;28mself\u001b[39m.num_samples % n\n\u001b[32m    198\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genesis/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected a 'cuda' device type for generator but found 'cpu'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ImageRotationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset folder containing images and .npy rotation matrices.\n",
    "            transform (callable, optional): Optional transform to be applied to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if fname.lower().endswith((\".npy\", \".jpg\", \".jpeg\")):\n",
    "                base = os.path.splitext(fname)[0]\n",
    "                img_path = os.path.join(self.root_dir, fname)\n",
    "                rot_path = os.path.join(self.root_dir, f\"{base.replace(\"image\", \"rotation\")}.th\")\n",
    "                if os.path.exists(rot_path):\n",
    "                    samples.append((img_path, rot_path))\n",
    "                \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, rot_path = self.samples[idx]\n",
    "        # Load image\n",
    "        image = np.load(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Load rotation matrix (3x3)\n",
    "        rotation_matrix = torch.load(rot_path)\n",
    "\n",
    "        return image, rotation_matrix\n",
    "\n",
    "def show_image(image_array):\n",
    "    \"\"\"\n",
    "    Display an image from a numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    image_array (np.ndarray): Image array with shape (height, width, channels)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image_array)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.title('Image Display')\n",
    "    plt.show()\n",
    "\n",
    "def create_dataloader(\n",
    "    root_dir, batch_size=32, shuffle=True, num_workers=0, img_size=224):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for (image, rotation_matrix) pairs.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(),\n",
    "    ])\n",
    "\n",
    "    dataset = ImageRotationDataset(root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader('dataset', batch_size=8)\n",
    "\n",
    "for images, rotations in dataloader:\n",
    "    print(\"Batch of images shape:\", images.shape)\n",
    "    print(\"Batch of rotation matrices shape:\", rotations.shape)\n",
    "    show_image(images[0].permute(1,2,0))\n",
    "    print(rotations[0])\n",
    "    break   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    dataloader = create_dataloader('dataset', batch_size=8)\n",
    "    deformnet = DeformNet()\n",
    "    optimizer = torch.optim.Adam(deformnet.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1):  # number of epochs\n",
    "        for i, (images, rotation_matrices) in tqdm(enumerate(dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = deformnet(images.to(\"cuda\"))\n",
    "            loss = criterion(outputs.to(\"cuda\"), rotation_matrices.to(\"cuda\"))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return deformnet\n",
    "\n",
    "\n",
    "trained_model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ce6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model, \"trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994ea88",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ca19",
   "metadata": {},
   "source": [
    "## Test scene setup (add second torus to evaluate overlapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06517797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 11/14/25 10:56:02.127 20040] [shell.py:_shell_pop_print@25] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;159m[Genesis] [10:56:03] [INFO] \u001b[38;5;121mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:03] [INFO] \u001b[38;5;121mâ”‚â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆ\u001b[0m\u001b[38;5;159m \u001b[38;5;121m\u001b[1m\u001b[3mGenesis\u001b[0m\u001b[38;5;159m \u001b[38;5;121mâ”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‰â”ˆâ”‚\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:03] [INFO] \u001b[38;5;121mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:04] [INFO] Running on \u001b[38;5;121m\u001b[4m[NVIDIA GeForce GTX 1650 Ti]\u001b[0m\u001b[38;5;159m with backend \u001b[38;5;121m\u001b[4mgs.cuda\u001b[0m\u001b[38;5;159m. Device memory: \u001b[38;5;121m\u001b[4m3.63\u001b[0m\u001b[38;5;159m GB.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:04] [INFO] ğŸš€ Genesis initialized. ğŸ”– version: \u001b[38;5;121m\u001b[4m0.3.6\u001b[0m\u001b[38;5;159m, ğŸ¨ theme: \u001b[38;5;121m\u001b[4mdark\u001b[0m\u001b[38;5;159m, ğŸŒ± seed: \u001b[38;5;121m\u001b[4m0\u001b[0m\u001b[38;5;159m, ğŸ› debug: \u001b[38;5;121m\u001b[4mFalse\u001b[0m\u001b[38;5;159m, ğŸ“ precision: \u001b[38;5;121m\u001b[4m32\u001b[0m\u001b[38;5;159m, ğŸï¸ performance: \u001b[38;5;121m\u001b[4mFalse\u001b[0m\u001b[38;5;159m, ğŸ’¬ verbose: \u001b[38;5;121m\u001b[4mINFO\u001b[0m\u001b[38;5;159m\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:06] [INFO] Scene \u001b[38;5;121m\u001b[3m<b425f10>\u001b[0m\u001b[38;5;159m created.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:06] [INFO] Adding \u001b[38;5;121m<gs.RigidEntity>\u001b[0m\u001b[38;5;159m. idx: \u001b[38;5;121m0\u001b[0m\u001b[38;5;159m, uid: \u001b[38;5;121m\u001b[3m<c71be91>\u001b[0m\u001b[38;5;159m, morph: \u001b[38;5;121m<gs.morphs.Plane>\u001b[0m\u001b[38;5;159m, material: \u001b[38;5;121m<gs.materials.Rigid>\u001b[0m\u001b[38;5;159m.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:06] [INFO] Adding \u001b[38;5;121m<gs.FEMEntity>\u001b[0m\u001b[38;5;159m. idx: \u001b[38;5;121m1\u001b[0m\u001b[38;5;159m, uid: \u001b[38;5;121m\u001b[3m<381e4c1>\u001b[0m\u001b[38;5;159m, morph: \u001b[38;5;121m<gs.morphs.Mesh(file='/home/flaccagora/Desktop/test/genesis/source/Torus.obj')>\u001b[0m\u001b[38;5;159m, material: \u001b[38;5;121m<gs.FEM.Muscle>\u001b[0m\u001b[38;5;159m.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:06] [INFO] Adding \u001b[38;5;121m<gs.FEMEntity>\u001b[0m\u001b[38;5;159m. idx: \u001b[38;5;121m2\u001b[0m\u001b[38;5;159m, uid: \u001b[38;5;121m\u001b[3m<e2a7b69>\u001b[0m\u001b[38;5;159m, morph: \u001b[38;5;121m<gs.morphs.Mesh(file='/home/flaccagora/Desktop/test/genesis/source/Torus.obj')>\u001b[0m\u001b[38;5;159m, material: \u001b[38;5;121m<gs.FEM.Muscle>\u001b[0m\u001b[38;5;159m.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:06] [INFO] Building scene \u001b[38;5;121m\u001b[3m<b425f10>\u001b[0m\u001b[38;5;159m...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:12] [INFO] Entity <381e4c1> added. class: FEMEntity, morph: Mesh, size: (2696, 773), material: <gs.FEM.Muscle>.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:13] [INFO] Entity <e2a7b69> added. class: FEMEntity, morph: Mesh, size: (2696, 773), material: <gs.FEM.Muscle>.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:56:13] [INFO] Compiling simulation kernels...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:57:06] [INFO] Building visualizer...\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [10:57:07] [INFO] Viewer created. Resolution: \u001b[38;5;121m800Ã—600\u001b[0m\u001b[38;5;159m, max_FPS: \u001b[38;5;121m60\u001b[0m\u001b[38;5;159m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import genesis as gs\n",
    "\n",
    "def gs_simul_setup():\n",
    "    ########################## init ##########################\n",
    "    gs.init(seed=0, precision='32', logging_level='info')\n",
    "\n",
    "    dt = 5e-4\n",
    "    scene = gs.Scene(\n",
    "        sim_options=gs.options.SimOptions(\n",
    "            substeps=10,\n",
    "            gravity=(0, 0, 0),\n",
    "        ),\n",
    "        viewer_options= gs.options.ViewerOptions(\n",
    "            camera_pos=(1.5, 0, 0.8),\n",
    "            camera_lookat=(0.0, 0.0, 0.0),\n",
    "            camera_fov=40,\n",
    "        ),\n",
    "        mpm_options=gs.options.MPMOptions(\n",
    "            dt=dt,\n",
    "            lower_bound=(-1.0, -1.0, -0.2),\n",
    "            upper_bound=( 1.0,  1.0,  1.0),\n",
    "        ),\n",
    "        fem_options=gs.options.FEMOptions(\n",
    "            dt=dt,\n",
    "        ),\n",
    "        vis_options=gs.options.VisOptions(\n",
    "            show_world_frame=False,\n",
    "        ),\n",
    "        show_viewer=True,\n",
    "    )\n",
    "\n",
    "    ########################## entities ##########################\n",
    "    scene.add_entity(morph=gs.morphs.Plane())\n",
    "\n",
    "    E, nu = 3.e4, 0.45\n",
    "    rho = 1000.\n",
    "\n",
    "\n",
    "    torus_fem_0 = scene.add_entity(\n",
    "        morph=gs.morphs.Mesh(\n",
    "            file='source/Torus.obj',\n",
    "            pos=(0.5, 0.4, 0.3),\n",
    "            scale=0.2,\n",
    "            ),\n",
    "        material=gs.materials.FEM.Muscle(\n",
    "            E=E,\n",
    "            nu=nu,\n",
    "            rho=rho,\n",
    "            model='stable-neohooken',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    torus_fem_1 = scene.add_entity(\n",
    "        morph=gs.morphs.Mesh(\n",
    "            file='source/Torus.obj',\n",
    "            pos=(0.5, 0.4, 0.3),\n",
    "            scale=0.2,\n",
    "            ),\n",
    "        material=gs.materials.FEM.Muscle(\n",
    "            E=E,\n",
    "            nu=nu,\n",
    "            rho=rho,\n",
    "            model='stable-neohooken',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    cam = scene.add_camera(\n",
    "        res    = (640, 480),\n",
    "        pos    = (3., 0.4, 0.3),\n",
    "        lookat = (0.5, 0.4, 0.3),\n",
    "        fov    = 30,\n",
    "        GUI    = False,\n",
    "    )\n",
    "\n",
    "\n",
    "    ########################## build ##########################\n",
    "    scene.build()\n",
    "\n",
    "    return scene, cam\n",
    "\n",
    "scene, cam = gs_simul_setup()\n",
    "torus_fem_0 = scene.entities[1]\n",
    "torus_fem_1 = scene.entities[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865034b",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab88ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "trained_model = DeformNet()\n",
    "trained_model = torch.load(\"trained.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c0fa7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import ImageRotationDataset\n",
    "import numpy as np\n",
    "\n",
    "def get_random_image():\n",
    "    dataset = ImageRotationDataset(\"dataset\")\n",
    "\n",
    "    image, rotation = dataset.samples[np.random.randint(len(dataset.samples))]\n",
    "    image = np.load(image)\n",
    "    rotation = torch.load(rotation)\n",
    "\n",
    "    return torch.tensor(image), rotation\n",
    "\n",
    "def get_predicted_rotation(image, trained_model):\n",
    "    image_pt = torch.tensor([image])\n",
    "    predicted_rotation = trained_model(image_pt)\n",
    "    return predicted_rotation.squeeze(0)\n",
    "\n",
    "# image, rotation = get_random_image()\n",
    "# pred_rotation = get_predicted_rotation(image,trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85c552c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m[Genesis] [11:03:52] [WARNING] Manally setting element positions. This is not recommended and could break gradient flow.\u001b[0m\n",
      "\u001b[38;5;226m[Genesis] [11:03:52] [WARNING] Manally setting element positions. This is not recommended and could break gradient flow.\u001b[0m\n",
      "\u001b[38;5;159m[Genesis] [11:03:52] [INFO] Running at \u001b[38;5;121m0.11\u001b[0m\u001b[38;5;159m FPS.\u001b[0m\n",
      "loss:  tensor(0.6179, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image, rotation = get_random_image()\n",
    "pred_rotation = get_predicted_rotation(image,trained_model)\n",
    "\n",
    "scene.reset()\n",
    "rotate_entity(torus_fem_0,rotation)\n",
    "rotate_entity(torus_fem_1,pred_rotation.to(\"cuda\"))\n",
    "scene.step()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "print(\"loss: \", criterion(rotation.to(\"cuda\"),pred_rotation.to(\"cuda\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f73246ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c527d31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 640, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = load_image(url)\n",
    "\n",
    "feature_extractor = pipeline(\n",
    "    model=\"facebook/dinov3-vitB16-pretrain-lvd1689m\",\n",
    "    task=\"image-feature-extraction\", \n",
    ")\n",
    "image, rotation = get_random_image()\n",
    "\n",
    "features = feature_extractor(to_pil_image(image.permute(2,0,1)))\n",
    "import torch \n",
    "torch.tensor(features).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50c2c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = load_image(url)\n",
    "\n",
    "pretrained_model_name = \"facebook/dinov3-vitB16-pretrain-lvd1689m\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pooled_output = outputs.pooler_output\n",
    "print(\"Pooled output shape:\", pooled_output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
